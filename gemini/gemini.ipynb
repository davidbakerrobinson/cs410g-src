{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12",
        "tags": []
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --user google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart current runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime kernel by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvKdaPDTznN"
      },
      "outputs": [],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "import time\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXHfaVS66_01"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lslYAvw37JGQ",
        "tags": [],
        "executionInfo": {
          "status": "ok",
          "timestamp": 1707714856049,
          "user_tz": 480,
          "elapsed": 49542,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY1nfXrqRxVX"
      },
      "source": [
        "### Load the Gemini Pro model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2998506fe6d1",
        "tags": [],
        "executionInfo": {
          "status": "ok",
          "timestamp": 1707714860105,
          "user_tz": 480,
          "elapsed": 181,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "model = GenerativeModel(\"gemini-pro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIl7R_jBUsaC"
      },
      "source": [
        "### Generate text from text prompts\n",
        "\n",
        "Send a text prompt to the model. The Gemini Pro (`gemini-pro`) model provides a streaming response mechanism. With this approach, you don't need to wait for the complete response; you can start processing fragments as soon as they're accessible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eAo-UsfZECGF",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1707714947375,
          "user_tz": 480,
          "elapsed": 824,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d5b1a2da-9cc4-47a5-bd20-b0fdb98e669e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Red\n",
            "2. Orange\n",
            "3. Yellow\n",
            "4. Green\n",
            "5. Blue\n",
            "6. Indigo\n",
            "7. Violet\n"
          ]
        }
      ],
      "source": [
        "response = model.generate_content(\"What are the colors of the rainbow?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us8idXnVyQ97"
      },
      "source": [
        "#### Try your own prompts\n",
        "\n",
        "- What are the biggest challenges facing the healthcare industry?\n",
        "- What are the latest developments in the automotive industry?\n",
        "- What are the biggest opportunities in retail industry?\n",
        "- (Try your own prompts!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmAZQW1GyQ97",
        "tags": []
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Create a numbered list of 10 items. Each item in the list should be a trend in the tech industry.\n",
        "Each trend should be less than 5 words.\"\"\"  # try your own prompt\n",
        "\n",
        "responses = model.generate_content(prompt, stream=True)\n",
        "\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDK4XLdz3Oqv"
      },
      "source": [
        "#### Model parameters\n",
        "\n",
        "Every prompt you send to the model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_2ann-F3WTo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "generation_config = GenerationConfig(\n",
        "    temperature=0.9,\n",
        "    top_p=1.0,\n",
        "    top_k=32,\n",
        "    candidate_count=1,\n",
        "    max_output_tokens=8192,\n",
        ")\n",
        "\n",
        "responses = model.generate_content(\n",
        "    \"What are the best hikes in Oregon?\",\n",
        "    generation_config=generation_config,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "common-cpu.m113",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m113"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}